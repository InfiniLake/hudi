/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.catalyst.expressions

import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.catalyst.expressions.AutoRecordKeyGenExpression.{escapeString, separator}
import org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper
import org.apache.spark.sql.catalyst.expressions.codegen.{CodeGenerator, CodegenContext, ExprCode, FalseLiteral}
import org.apache.spark.sql.types.{DataType, StringType}
import org.apache.spark.unsafe.types.UTF8String

/**
 * Expression providing for automatic generation of the record-keys for every row w/in the
 * used [[org.apache.spark.sql.DataFrame]], bearing following properties:
 *
 * <ol>
 *   <li>Generated keys will be unique not only w/in provided [[org.apache.spark.sql.DataFrame]], but
 *   globally unique w/in the target table</li>
 *   <li>Generated keys have minimal overhead (to compute, persist and read)</li>
 * </ol>
 *
 * Keys adhere to the following format:
 *
 * {@code [nonce]_[batch_row_id]}
 *
 * Where
 * <ul>
 *   <li>[[nonce]]: is a nonce allowing to uniquely identify this particular batch (for ex, could be commit-id)</li>
 *   <li>[[batch_row_id]]: unique identity value generating by the way of enumeration records w/in
 *   individual Spark partitions</li>
 * </ul>
 *
 * NOTE: Keys generated by this expression are very similar to the [[seqNo]] meta-field
 */
case class AutoRecordKeyGenExpression(nonce: String) extends LeafExpression with Stateful {

  /**
   * Record ID within each partition. By being transient, count's value is reset to 0 every time
   * we serialize and deserialize and initialize it.
   */
  @transient private[this] var count: Long = _
  @transient private[this] var partitionPrefix: UTF8String = _

  override protected def initializeInternal(partitionIndex: Int): Unit = {
    count = 0L
    partitionPrefix = UTF8String.fromString(s"${nonce}_${partitionIndex}_")
  }

  override def nullable: Boolean = false

  override def dataType: DataType = StringType

  override protected def evalInternal(input: InternalRow): UTF8String = {
    val currentCount = count
    count += 1

    UTF8String.concat(partitionPrefix, UTF8String.fromString(currentCount.toString))
  }

  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
    val countTerm = ctx.addMutableState(CodeGenerator.JAVA_LONG, "count")
    val partitionPrefixTerm = "partitionPrefix"
    ctx.addImmutableStateIfNotExists(CodeGenerator.javaType(StringType), partitionPrefixTerm)
    ctx.addPartitionInitializationStatement(s"$countTerm = 0L;")
    ctx.addPartitionInitializationStatement(s"$partitionPrefixTerm = " +
      s"UTF8String.fromString(${escapeString(nonce + separator)} + String.valueOf(partitionIndex) + ${escapeString(separator)});")

    ev.copy(code = code"""
      final ${CodeGenerator.javaType(dataType)} ${ev.value} = UTF8String.concat($partitionPrefixTerm, UTF8String.fromString(String.valueOf($countTerm)));
      $countTerm++;""", isNull = FalseLiteral)
  }

  override def nodeName: String = "record_key_gen_expression"

  override def sql: String = s"record_key_gen_expression()"

  override def freshCopy(): AutoRecordKeyGenExpression = AutoRecordKeyGenExpression(nonce)
}

object AutoRecordKeyGenExpression {

  private val separator = "_"

  private def escapeString(s: String): String = "\"" + s + "\""

}
